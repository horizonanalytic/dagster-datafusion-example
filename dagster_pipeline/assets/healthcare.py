"""
Healthcare Domain Assets

Demonstrates generating healthcare organization data in Parquet format.
Matches the schema patterns used in the companion datafusion_service.
"""

from dagster import (
    asset,
    DailyPartitionsDefinition,
    StaticPartitionsDefinition,
    MultiPartitionsDefinition,
    AssetExecutionContext,
    MaterializeResult,
    MetadataValue,
    AssetKey,
)
import pandas as pd
import random
from datetime import datetime, timedelta
from typing import List


# ========== PARTITION DEFINITIONS ==========

# Daily partitions starting from 2024-01-01
daily_partitions = DailyPartitionsDefinition(start_date="2024-01-01")

# US state partitions (subset for demo)
state_partitions = StaticPartitionsDefinition([
    "AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA",
    "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD",
    "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ",
    "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC",
    "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"
])

# Multi-dimensional partitioning: state + date
# This creates HIVE-style partitioning for DataFusion partition pruning
multi_partitions = MultiPartitionsDefinition({
    "date": daily_partitions,
    "state": state_partitions,
})


# ========== HELPER FUNCTIONS ==========

def generate_organization_name(org_id: int, state: str) -> str:
    """Generate realistic organization names"""
    prefixes = ["City", "County", "Regional", "Community", "University", "St.", "Memorial"]
    suffixes = ["Hospital", "Medical Center", "Health System", "Clinic", "Healthcare"]

    prefix = random.choice(prefixes)
    suffix = random.choice(suffixes)

    return f"{state} {prefix} {suffix} #{org_id}"


def generate_organizations_data(
    num_rows: int = 10000,
    states: List[str] = None,
    date: str = None
) -> pd.DataFrame:
    """
    Generate synthetic healthcare organization data

    Schema matches datafusion_service expectations:
    - grouping_id: Unique organization identifier
    - grouping_name: Organization name
    - state: US state code
    - all_providers_primary: Provider count
    - npi_count: NPI count
    - last_updated: Timestamp
    """
    if states is None:
        states = ["CA", "NY", "TX", "FL", "IL"]

    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")

    data = {
        "grouping_id": range(1, num_rows + 1),
        "grouping_name": [
            generate_organization_name(i, random.choice(states))
            for i in range(1, num_rows + 1)
        ],
        "state": [random.choice(states) for _ in range(num_rows)],
        "all_providers_primary": [random.randint(1, 500) for _ in range(num_rows)],
        "npi_count": [random.randint(1, 1000) for _ in range(num_rows)],
        "tax_id": [f"{random.randint(10, 99)}-{random.randint(1000000, 9999999)}" for _ in range(num_rows)],
        "created_date": [date] * num_rows,
        "last_updated": [datetime.now().isoformat()] * num_rows,
    }

    return pd.DataFrame(data)


# ========== ASSETS ==========

@asset(
    key=AssetKey(["Silver", "TaxID", "raw_organizations"]),
    io_manager_key="parquet_io_manager",
    group_name="healthcare",
    compute_kind="python",
)
def raw_organizations(context: AssetExecutionContext) -> MaterializeResult:
    """
    Extract raw healthcare organizations data

    Generates 10,000 synthetic organization records with:
    - Organization IDs and names
    - US state codes
    - Provider counts
    - NPI counts
    - Timestamps

    Output: s3://bucket/Silver/TaxID/raw_organizations.parquet

    This matches the datafusion_service asset naming pattern:
    - Asset key: ["Silver", "TaxID", "raw_organizations"]
    - Schema: uat_silver_tax_id (auto-generated by datafusion_service)
    - Table: raw_organizations
    """
    context.log.info("Generating raw organizations data...")

    # Generate synthetic data
    df = generate_organizations_data(
        num_rows=10000,
        states=["CA", "NY", "TX", "FL", "IL", "PA", "OH", "GA", "NC", "MI"]
    )

    context.log.info(f"Generated {len(df):,} organization records")

    # Calculate statistics
    state_counts = df['state'].value_counts()
    avg_providers = df['all_providers_primary'].mean()
    total_npis = df['npi_count'].sum()

    # Return DataFrame with comprehensive metadata
    return MaterializeResult(
        asset_key=AssetKey(["Silver", "TaxID", "raw_organizations"]),
        metadata={
            # Standard Dagster metadata (renders as time series)
            "dagster/row_count": len(df),

            # Custom metadata
            "total_organizations": MetadataValue.int(len(df)),
            "unique_states": MetadataValue.int(len(state_counts)),
            "avg_providers_per_org": MetadataValue.float(round(avg_providers, 2)),
            "total_npis": MetadataValue.int(int(total_npis)),

            # State distribution (top 5)
            "state_distribution": MetadataValue.md(
                state_counts.head(5).to_markdown()
            ),

            # Schema preview
            "schema": MetadataValue.md(
                df.dtypes.to_frame('dtype').to_markdown()
            ),

            # Data preview
            "preview": MetadataValue.md(
                df.head(10).to_markdown()
            ),
        }
    )


@asset(
    key=AssetKey(["Silver", "TaxID", "partitioned_organizations"]),
    partitions_def=multi_partitions,
    io_manager_key="partitioned_parquet_io_manager",
    group_name="healthcare",
    compute_kind="python",
)
def partitioned_organizations(
    context: AssetExecutionContext,
    raw_organizations: pd.DataFrame,
) -> MaterializeResult:
    """
    Partition organizations by state and date (HIVE-style)

    This creates multi-dimensional partitioning:
    - Partition by STATE (CA, NY, TX, etc.)
    - Partition by DATE (2024-01-01, 2024-01-02, etc.)

    Output structure:
    s3://bucket/Silver/TaxID/partitioned_organizations/
      state=CA/
        date=2024-01-15/
          data.parquet
        date=2024-01-16/
          data.parquet
      state=NY/
        date=2024-01-15/
          data.parquet

    DataFusion Query Benefits:

    Query:  SELECT * FROM partitioned_organizations WHERE state = 'CA'
    Result: Only reads state=CA/* partitions (skips NY, TX, etc.)

    Query:  SELECT * FROM partitioned_organizations
            WHERE state = 'CA' AND date = '2024-01-15'
    Result: Only reads 1 partition file (10-100x faster!)

    This is the KEY optimization for DataFusion query performance!
    """
    # Get partition keys
    partition_key = context.partition_key.keys_by_dimension
    partition_date = partition_key["date"]
    partition_state = partition_key["state"]

    context.log.info(f"Processing partition: state={partition_state}, date={partition_date}")

    # Filter raw data for this specific partition
    filtered_df = raw_organizations[
        (raw_organizations['state'] == partition_state) &
        (raw_organizations['created_date'] == partition_date)
    ].copy()

    # If no data for this partition, generate some
    if len(filtered_df) == 0:
        context.log.warning(f"No data for state={partition_state}, date={partition_date}. Generating synthetic data.")
        filtered_df = generate_organizations_data(
            num_rows=random.randint(50, 500),
            states=[partition_state],
            date=partition_date
        )

    # Add partition columns (required for HIVE partitioning)
    filtered_df['partition_date'] = partition_date
    filtered_df['partition_state'] = partition_state

    context.log.info(f"Partitioned data: {len(filtered_df):,} rows for {partition_state} on {partition_date}")

    return MaterializeResult(
        metadata={
            "partition_date": partition_date,
            "partition_state": partition_state,
            "row_count": len(filtered_df),
            "total_providers": MetadataValue.int(int(filtered_df['all_providers_primary'].sum())),
            "avg_providers": MetadataValue.float(round(filtered_df['all_providers_primary'].mean(), 2)),
            "processing_timestamp": datetime.now().isoformat(),
        }
    )


@asset(
    key=AssetKey(["Gold", "Analytics", "daily_org_aggregates"]),
    partitions_def=daily_partitions,
    io_manager_key="parquet_io_manager",
    group_name="healthcare",
    compute_kind="python",
)
def daily_org_aggregates(
    context: AssetExecutionContext,
) -> MaterializeResult:
    """
    Daily aggregations of organization metrics across all states

    Generates time-series data for analytics dashboards:
    - Organizations by state (daily snapshot)
    - Provider counts by state
    - NPI counts by state

    Output: s3://bucket/Gold/Analytics/daily_org_aggregates/{date}.parquet

    Use case: Track organization growth over time, identify trends
    """
    partition_date = context.partition_key

    context.log.info(f"Generating daily aggregates for {partition_date}")

    # Generate aggregate data (in production, this would query partitioned_organizations)
    states = ["CA", "NY", "TX", "FL", "IL", "PA", "OH", "GA", "NC", "MI"]

    aggregates = {
        "date": [partition_date] * len(states),
        "state": states,
        "org_count": [random.randint(100, 2000) for _ in states],
        "total_providers": [random.randint(1000, 50000) for _ in states],
        "total_npis": [random.randint(2000, 100000) for _ in states],
        "avg_providers_per_org": [round(random.uniform(10, 250), 2) for _ in states],
    }

    df = pd.DataFrame(aggregates)

    # Calculate summary statistics
    total_orgs = df['org_count'].sum()
    total_providers = df['total_providers'].sum()
    top_state = df.nlargest(1, 'org_count').iloc[0]['state']

    context.log.info(f"Aggregated {total_orgs:,} orgs, {total_providers:,} providers across {len(states)} states")

    return MaterializeResult(
        metadata={
            "date": partition_date,
            "total_organizations": MetadataValue.int(int(total_orgs)),
            "total_providers": MetadataValue.int(int(total_providers)),
            "states_processed": MetadataValue.int(len(states)),
            "top_state": top_state,
            "aggregates_preview": MetadataValue.md(df.to_markdown()),
        }
    )


# ========== ASSET GROUP METADATA ==========

# These assets form the "healthcare" domain in the Dagster UI
# Users can materialize them together as a group
